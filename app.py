import streamlit as st
import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from gensim import corpora, models
import pandas as pd
import chardet
import os
import requests
import tempfile
import networkx as nx
from collections import defaultdict

# ç¼“å­˜å­—ä½“ä¸‹è½½å‡½æ•°
@st.cache_resource
def download_font():
    font_url = "https://github.com/StellarCN/scp_zh/raw/master/fonts/SimHei.ttf"
    try:
        response = requests.get(font_url)
        response.raise_for_status()
        temp_font = tempfile.NamedTemporaryFile(delete=False, suffix=".ttf")
        temp_font.write(response.content)
        temp_font.close()
        return temp_font.name
    except Exception as e:
        st.error(f"å­—ä½“ä¸‹è½½å¤±è´¥: {str(e)}")
        return None

# ================== åˆå§‹åŒ–å…¨å±€ç»„ä»¶ ==================
st.title("ğŸ“– å°è¯´æ–‡æœ¬åˆ†æç³»ç»Ÿ")
uploaded_file = st.file_uploader("ä¸Šä¼ å°è¯´æ–‡æœ¬æ–‡ä»¶ï¼ˆ.txtï¼‰", type="txt")

# ================== å·¥å…·å‡½æ•°å®šä¹‰ ==================
def find_chinese_font():
    font_path = download_font()
    if font_path:
        return os.path.abspath(font_path)
    st.error("âŒ æ— æ³•è·å–ä¸­æ–‡å­—ä½“æ–‡ä»¶")
    st.stop()

def generate_wordcloud(text):
    try:
        font_path = find_chinese_font()
        wc = WordCloud(
            font_path=font_path,
            width=800,
            height=600,
            background_color="white",
            collocations=False,
            max_words=200,
            prefer_horizontal=0.9
        )
        wordcloud = wc.generate(text)
        fig, ax = plt.subplots()
        ax.imshow(wordcloud)
        ax.axis("off")
        st.pyplot(fig)
    except Exception as e:
        st.error(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {str(e)}")

def safe_decode(raw_data):
    try:
        detection = chardet.detect(raw_data)
        encoding = detection['encoding'] or 'utf-8'
        return raw_data.decode(encoding, errors='replace')
    except Exception as e:
        return raw_data.decode('utf-8', errors='replace')

# æ–°å¢äººç‰©å…³ç³»åˆ†æå‡½æ•°
def analyze_relationships(text, target_character):
    # æ·»åŠ è‡ªå®šä¹‰è¯å…¸æé«˜äººç‰©è¯†åˆ«
    jieba.add_word(target_character)
    
    # æ„å»ºå…±ç°å…³ç³»
    relationships = defaultdict(int)
    window_size = 5  # å…±ç°çª—å£å¤§å°
    
    words = jieba.lcut(text)
    for i in range(len(words)):
        if words[i] == target_character:
            start = max(0, i - window_size)
            end = min(len(words), i + window_size)
            neighbors = words[start:end]
            for char in neighbors:
                if char != target_character and len(char) > 1:  # è¿‡æ»¤å•å­—å’Œéç›®æ ‡äººç‰©
                    relationships[char] += 1
    
    return dict(relationships)

def plot_network(target_character, relationships):
    G = nx.Graph()
    G.add_node(target_character)
    
    for char, weight in relationships.items():
        G.add_node(char)
        G.add_edge(target_character, char, weight=weight)
    
    pos = nx.spring_layout(G)
    plt.figure(figsize=(10,8))
    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='skyblue')
    nx.draw_networkx_edges(G, pos, width=1.5, alpha=0.6)
    nx.draw_networkx_labels(G, pos, font_family='SimHei', font_size=12)
    
    edge_labels = {(u, v): d['weight'] for u, v, d in G.edges(data=True)}
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
    
    plt.title(f"'{target_character}' äººç‰©å…³ç³»ç½‘ç»œ")
    plt.axis("off")
    st.pyplot(plt)

# ================== ä¸»å¤„ç†é€»è¾‘ ==================
if uploaded_file is not None:
    try:
        raw_data = uploaded_file.read()
        text = safe_decode(raw_data)
        words = jieba.lcut(text)

        # é«˜é¢‘è¯ç»Ÿè®¡
        st.subheader("ğŸ“Š é«˜é¢‘è¯æ±‡ç»Ÿè®¡")
        word_freq = pd.Series(words).value_counts().head(20)
        st.dataframe(word_freq)

        # è¯äº‘ç”Ÿæˆ
        st.subheader("â˜ è¯äº‘å›¾")
        generate_wordcloud(" ".join(words))

        # LDAåˆ†æ
        st.subheader("ğŸ¯ ä¸»é¢˜å…³é”®è¯æå–")
        try:
            dictionary = corpora.Dictionary([words])
            corpus = [dictionary.doc2bow(words)]
            lda = models.LdaModel(corpus, num_topics=3, id2word=dictionary)
            for topic in lda.print_topics():
                st.code(f"ä¸»é¢˜ {topic[0]}: {topic[1]}")
        except Exception as e:
            st.error(f"ä¸»é¢˜åˆ†æå¤±è´¥: {str(e)}")

        # æ–°å¢äººç‰©å…³ç³»åˆ†ææ¨¡å—
        st.subheader("ğŸ•¸ï¸ äººç‰©å…³ç³»ç½‘ç»œåˆ†æ")
        target_character = st.text_input("è¾“å…¥è¦åˆ†æçš„äººç‰©åç§°ï¼ˆå¦‚ï¼šæ®µèª‰ï¼‰:")
        if target_character:
            if target_character not in text:
                st.warning("âš ï¸ è¯¥äººç‰©ä¸å­˜åœ¨äºæ–‡æœ¬ä¸­")
            else:
                relationships = analyze_relationships(text, target_character)
                if not relationships:
                    st.warning("æœªæ‰¾åˆ°ç›¸å…³äººç‰©å…³ç³»")
                else:
                    plot_network(target_character, relationships)
                    st.info(f"å…±æ‰¾åˆ° {len(relationships)} ä¸ªå…³è”äººç‰©")

    except Exception as e:
        st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        st.stop()
else:
    st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ æ–‡æœ¬æ–‡ä»¶")