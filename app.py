import streamlit as st
import jieba
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from gensim import corpora, models
import pandas as pd
import chardet
import os
import requests
import tempfile

# ç¼“å­˜å­—ä½“ä¸‹è½½å‡½æ•°
@st.cache_resource
def download_font():
    font_url = "https://github.com/StellarCN/scp_zh/raw/master/fonts/SimHei.ttf"
    try:
        response = requests.get(font_url)
        response.raise_for_status()
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜å­—ä½“
        temp_font = tempfile.NamedTemporaryFile(delete=False, suffix=".ttf")
        temp_font.write(response.content)
        temp_font.close()
        return temp_font.name
    except Exception as e:
        st.error(f"å­—ä½“ä¸‹è½½å¤±è´¥: {str(e)}")
        return None

# ================== åˆå§‹åŒ–å…¨å±€ç»„ä»¶ ==================
st.title("ğŸ“– å°è¯´æ–‡æœ¬åˆ†æç³»ç»Ÿ")
uploaded_file = st.file_uploader("ä¸Šä¼ å°è¯´æ–‡æœ¬æ–‡ä»¶ï¼ˆ.txtï¼‰", type="txt")

# ================== å·¥å…·å‡½æ•°å®šä¹‰ ==================
def find_chinese_font():
    """å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°å­—ä½“æ–‡ä»¶ï¼ˆç»ˆæè§£å†³æ–¹æ¡ˆï¼‰"""
    font_path = download_font()
    if font_path:
        return os.path.abspath(font_path)
    st.error("âŒ æ— æ³•è·å–ä¸­æ–‡å­—ä½“æ–‡ä»¶")
    st.stop()

def generate_wordcloud(text):
    """å®Œå…¨ä¾èµ–æœ¬åœ°å­—ä½“çš„è¯äº‘ç”Ÿæˆå‡½æ•°"""
    try:
        # è·å–ç»å¯¹å­—ä½“è·¯å¾„
        font_path = find_chinese_font()

        # åˆ›å»ºè¯äº‘å¯¹è±¡ï¼ˆå¼ºåˆ¶ä½¿ç”¨æŒ‡å®šå­—ä½“ï¼‰
        wc = WordCloud(
            font_path=font_path,
            width=800,
            height=600,
            background_color="white",
            collocations=False,
            max_words=200,
            prefer_horizontal=0.9  # ä¼˜åŒ–ä¸­æ–‡æ˜¾ç¤º
        )

        # ç”Ÿæˆå¹¶æ˜¾ç¤ºè¯äº‘
        wordcloud = wc.generate(text)
        fig, ax = plt.subplots()
        ax.imshow(wordcloud)
        ax.axis("off")
        st.pyplot(fig)

        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
        st.caption(f"å½“å‰ä½¿ç”¨å­—ä½“è·¯å¾„: {font_path}")

    except Exception as e:
        st.error(f"è¯äº‘ç”Ÿæˆå¤±è´¥: {str(e)}")
        st.image("https://via.placeholder.com/800x600.png?text=è¯äº‘ç”Ÿæˆå¤±è´¥",
                 use_column_width=True,
                 caption="ç”Ÿæˆå¤±è´¥ç¤ºä¾‹å›¾")

def safe_decode(raw_data):
    """å®‰å…¨è§£ç å®ç°ï¼ˆä¿æŒåŸæ ·ï¼‰"""
    try:
        detection = chardet.detect(raw_data)
        encoding = detection['encoding'] or 'utf-8'
        return raw_data.decode(encoding, errors='replace')
    except Exception as e:
        st.error(f"è§£ç å¤±è´¥: {str(e)}")
        return raw_data.decode('utf-8', errors='replace')

# ================== ä¸»å¤„ç†é€»è¾‘ ==================
if uploaded_file is not None:
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        raw_data = uploaded_file.read()
        text = safe_decode(raw_data)

        # 1. åˆ†è¯å¤„ç†
        words = jieba.lcut(text)
        if len(words) == 0:
            raise ValueError("åˆ†è¯ç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹")

        # 2. é«˜é¢‘è¯ç»Ÿè®¡
        st.subheader("ğŸ“Š é«˜é¢‘è¯æ±‡ç»Ÿè®¡")
        word_freq = pd.Series(words).value_counts().head(20)
        st.dataframe(word_freq)

        # 3. ç”Ÿæˆè¯äº‘ï¼ˆå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°å­—ä½“æ–¹æ¡ˆï¼‰
        st.subheader("â˜ è¯äº‘å›¾")
        generate_wordcloud(" ".join(words))

        # 4. LDAåˆ†æï¼ˆä¿æŒåŸæ ·ï¼‰
        st.subheader("ğŸ¯ ä¸»é¢˜å…³é”®è¯æå–")
        try:
            dictionary = corpora.Dictionary([words])
            corpus = [dictionary.doc2bow(words)]
            lda = models.LdaModel(corpus, num_topics=3, id2word=dictionary)
            for topic in lda.print_topics():
                st.code(f"ä¸»é¢˜ {topic[0]}: {topic[1]}")
        except Exception as e:
            st.error(f"ä¸»é¢˜åˆ†æå¤±è´¥: {str(e)}")

    except Exception as e:
        st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
        st.stop()
else:
    st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ æ–‡æœ¬æ–‡ä»¶")
    